# 96 vCPU Single-Node Results (3x 2-min tests)

See [run details](./run.md) for the full benchmark run output.

- âœï¸ write: 19.67 MiB/s (20,144 msg/s)
- ðŸ“–ï¸ read: 19.67 MiB/s (20,138 msg/s)
- write latency:
  - p99: 9.42 ms
  - p95: 1.99 ms
  - p50: 1.16 ms
- read latency:
  - p99: 22.59 ms
  - p95: 14.93 ms
  - p50: 6.27 ms
- end-to-end latency:
  - p99: 930.09 ms (see [High E2E Latency Note](../../IMPERFECTIONS.md#high-e2e-latency-note))
  - p95: 709.01 ms
  - p50: 12.59 ms
- bottleneck: ?
  - I didn't push it further. It's possible more clients would have bumped up the throughput slightly, but it traded off a lot of CPU for relatively little gain.

- server: `c7i.24xlarge` (96 vCPU, 192 GiB RAM)
  - io2 50GB /w 12_000 IOPS
  - Ubuntu 24.04
  - modified (upscaled) Postgres settings with huge_pages enabled
  - CPU usage: ~40-60% CPU
  - us-east-1a 

- test driver client: `c7i.4xlarge`
  - 100 writers
  - 200 readers
  - 1 KB payload
  - 2 minute duration
  - us-east-1a


In this test, it was time to reconfigure postgres. It's well known that its default settings are very conservative:
> The Postgres defaults are very conservative. It will start even on a tiny machine like Raspberry Pi. Which is great! The flip side is itâ€™s terrible for typical database servers which tend to have much more RAM/CPU.
> To get good performance on such larger machines, you need to adjust a couple parameters (shared_buffers, max_wal_size, â€¦).

We are running on a 96 vCPU machine with 128GB of RAM after all. I talked to ChatGPT a ton and it gave me the following configs:
- [postgres config](./modified_postgresql.conf) - it suggested I substantially increase some settings like `shared_buffers` and `max_worker_processes`.
- [huge pages](./change_configs.md#5-disable-transparent-huge-pages-in-the-kernel) - it suggested I modify the kernel's huge page count and configurations
- `queue` [table-specific vacuum settings](./table_vacuum_tuning.md) - it suggested more aggressive vacuuming for the `queue` table
The way I applied these changes can be found in the [change configs script](./change_configs.md).

Despite that - this run wasn't that impressive.

- There is some bottleneck in this _single-table_ queue approach at this scale that I didn't figure out with the limited time I spent on it.
- Adding more writers/readers didn't boost throughput. It only used more CPU on the server.
- In a previous undocumented test, I managed to get ~15.6k msg/s (16 MiB/s) from a **32 vCPU** server; This shows the 96 vCPU is overkill here.
- I figured that it wasn't important to reach absurd numbers on a single table, since all realistic scenarios would have multiple queues and never reach 20,000 msg/s on a single one.
- I unfortunately didn't test further with more tables. While the CPU was high, I don't think it would have further increased if the clients were simply directed to other queues
- If you're interested, maybe tweak the code, run a test and contribute a PR back?

On the flip-side, I did run with multiple tables on the **pub-sub** workload. Check the [pub-sub 96 vCPU results](../../../pubsub/96vcpu/single_node/96vcpu.md) for some impressive numbers...

# Run

```bash
./pg_msg_bench --host=$HOST --port=5432 --db=benchmark --user=postgres --password=postgres  \
 --duration=120s --payload=1024 --report=5s \
 --writers=100 --readers=200 \
 --throttle_writes=20000 --mode=queue \
 --tune-table-vacuum
```
